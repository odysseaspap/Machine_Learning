{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming assignment 11: Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework sheet we will implement Expectation-Maximization algorithm for learning & inference in a Gaussian mixture model.\n",
    "\n",
    "We will use the [dataset](http://www.stat.cmu.edu/~larry/all-of-statistics/=data/faithful.dat) containing information about eruptions of a geyser called \"Old Faithful\". \n",
    "\n",
    "As usual, your task is to fill out the missing code, run the notebook, convert it to PDF and attach it you your HW solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.loadtxt('faithful.txt')\n",
    "plt.figure(figsize=[6, 6])\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.xlabel('Eruptions (minutes)')\n",
    "plt.ylabel('Waiting time (minutes)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Normalize the data\n",
    "\n",
    "Notice, how the values on two axes are on very different scales. This might cause problems for our clustering algorithm. \n",
    "\n",
    "Normalize the data, such that it lies in the range $[0, 1]$ along each dimension (each column of X)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_data(X):\n",
    "    \"\"\"Normalize data such that it lies in range [0, 1] along every dimension.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array, shape [N, D]\n",
    "        Data matrix, each row represents a sample.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    X_norm : np.array, shape [N, D]\n",
    "        Normalized data matrix. \n",
    "    \"\"\"\n",
    "    return (X - X.min(0)) / (X.max(0) - X.min(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[6, 6])\n",
    "X_norm = normalize_data(X)\n",
    "plt.scatter(X_norm[:, 0], X_norm[:, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Compute the log-likelihood of GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gmm_log_likelihood(X, means, covs, mixing_coefs):\n",
    "    \"\"\"Compute the log-likelihood of the data under current parameters setting.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array, shape [N, D]\n",
    "        Data matrix with samples as rows.\n",
    "    means : np.array, shape [K, D]\n",
    "        Means of the GMM (\\mu in lecture notes).\n",
    "    covs : np.array, shape [K, D, D]\n",
    "        Covariance matrices of the GMM (\\Sigma in lecuture notes).\n",
    "    mixing_coefs : np.array, shape [K]\n",
    "        Mixing proportions of the GMM (\\pi in lecture notes).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    log_likelihood : float\n",
    "        log p(X | \\mu, \\Sigma, \\pi) - Log-likelihood of the data under the given GMM.\n",
    "    \"\"\"\n",
    "    num_components = len(mixing_coefs)\n",
    "    sample_likelihoods = np.zeros(X.shape[0])\n",
    "    for k in range(num_components):\n",
    "        sample_likelihoods += multivariate_normal.pdf(X, means[k], covs[k]) * mixing_coefs[k]\n",
    "    return np.sum(np.log(sample_likelihoods))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: E step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def e_step(X, means, covs, mixing_coefs):\n",
    "    \"\"\"Perform the E step.\n",
    "    \n",
    "    Compute the responsibilities.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array, shape [N, D]\n",
    "        Data matrix with samples as rows.\n",
    "    means : np.array, shape [K, D]\n",
    "        Means of the GMM (\\mu in lecture notes).\n",
    "    covs : np.array, shape [K, D, D]\n",
    "        Covariance matrices of the GMM (\\Sigma in lecuture notes).\n",
    "    mixing_coefs : np.array, shape [K]\n",
    "        Mixing proportions of the GMM (\\pi in lecture notes).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    responsibilities : np.array, shape [N, K]\n",
    "        Cluster responsibilities for the given data.\n",
    "    \"\"\"\n",
    "    R = np.hstack([(multivariate_normal.pdf(X_norm, means[k], covs[k])).reshape(-1, 1) \n",
    "                   for k in range(len(mixing_coefs))])\n",
    "    RR = R * mixing_coefs[None, :]\n",
    "    return RR / RR.sum(1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: M step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def m_step(X, responsibilities):\n",
    "    \"\"\"Update the parameters \\theta of the GMM to maximize E[log p(X, Z | \\theta)].\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array, shape [N, D]\n",
    "        Data matrix with samples as rows.\n",
    "    responsibilities : np.array, shape [N, K]\n",
    "        Cluster responsibilities for the given data.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    means : np.array, shape [K, D]\n",
    "        Means of the GMM (\\mu in lecture notes).\n",
    "    covs : np.array, shape [K, D, D]\n",
    "        Covariance matrices of the GMM (\\Sigma in lecuture notes).\n",
    "    mixing_coefs : np.array, shape [K]\n",
    "        Mixing proportions of the GMM (\\pi in lecture notes).\n",
    "    \n",
    "    \"\"\"\n",
    "    class_counts = responsibilities.sum(0)\n",
    "    num_components = responsibilities.shape[1]\n",
    "    covs = []\n",
    "    means = np.dot(responsibilities.T, X) / class_counts[:, None]\n",
    "    for k in range(num_components):\n",
    "        sigma_k = (responsibilities[:, k][:, None] * (X - means[k])).T.dot(X - means[k]) / class_counts[k]\n",
    "        covs.append(sigma_k)\n",
    "    mixing_coefs = class_counts / class_counts.sum()\n",
    "    return means, covs, mixing_coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the result (nothing to do here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_gmm_2d(X, responsibilities, means, covs, mixing_coefs):\n",
    "    \"\"\"Visualize a mixture of 2 bivariate Gaussians.\n",
    "    \n",
    "    This is badly written code. Please don't write code like this.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=[6, 6])\n",
    "    palette = np.array(sns.color_palette('colorblind', n_colors=3))[[0, 2]]\n",
    "    colors = responsibilities.dot(palette)\n",
    "    # Plot the samples colored according to p(z|x)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=colors, alpha=0.5)\n",
    "    # Plot locations of the means\n",
    "    for ix, m in enumerate(means):\n",
    "        plt.scatter(m[0], m[1], s=300, marker='X', c=palette[ix],\n",
    "                    edgecolors='k', linewidths=1,)\n",
    "    # Plot contours of the Gaussian\n",
    "    x = np.linspace(0, 1, 50)\n",
    "    y = np.linspace(0, 1, 50)\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    for k in range(len(mixing_coefs)):\n",
    "        zz = mlab.bivariate_normal(xx, yy, np.sqrt(covs[k][0, 0]),\n",
    "                                   np.sqrt(covs[k][1, 1]), \n",
    "                                   means[k][0], means[k][1], covs[k][0, 1])\n",
    "        plt.contour(xx, yy, zz, 2, colors='k')\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the EM algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_norm = normalize_data(X)\n",
    "max_iters = 20\n",
    "\n",
    "# Initialize the parameters\n",
    "means = np.array([[0.2, 0.6], [0.8, 0.4]])\n",
    "covs = np.array([0.5 * np.eye(2), 0.5 * np.eye(2)])\n",
    "mixing_coefs = np.array([0.5, 0.5])\n",
    "\n",
    "old_log_likelihood = gmm_log_likelihood(X_norm, means, covs, mixing_coefs)\n",
    "responsibilities = e_step(X_norm, means, covs, mixing_coefs)\n",
    "print('At initialization: log-likelihood = {0}'\n",
    "      .format(old_log_likelihood))\n",
    "plot_gmm_2d(X_norm, responsibilities, means, covs, mixing_coefs)\n",
    "\n",
    "# Perform the EM iteration\n",
    "for i in range(max_iters):\n",
    "    responsibilities = e_step(X_norm, means, covs, mixing_coefs)\n",
    "    means, covs, mixing_coefs = m_step(X_norm, responsibilities)\n",
    "    new_log_likelihood = gmm_log_likelihood(X_norm, means, covs, mixing_coefs)\n",
    "    # Report & visualize the optimization progress\n",
    "    print('Iteration {0}: log-likelihood = {1:.2f}, improvement = {2:.2f}'\n",
    "          .format(i, new_log_likelihood, new_log_likelihood - old_log_likelihood))\n",
    "    old_log_likelihood = new_log_likelihood\n",
    "    plot_gmm_2d(X_norm, responsibilities, means, covs, mixing_coefs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
